{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef51b239",
   "metadata": {},
   "source": [
    "# Testing Wikipedia Retreiver\n",
    "\n",
    "# Use Case : Get the information from wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "843070b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "Content:\n",
      "The United States had been providing military aid and economic assistance to Pakistan for various purposes since 1948. Recently U.S. stopped military aid to Pakistan, which was about US$2 billion per year. With U.S. military assistance suspended in 2018 and civilian aid reduced to about $300 million for 2022, Pakistani authorities have turned to other countries for help.\n",
      "\n",
      "\n",
      "== History ==\n",
      "From 1947 to 1958, under civilian leadership, the United States provided Pakistan with modest economic aid and limited military assistance. During this period, Pakistan became a member of the South East Asian Treaty Organization (SEATO) and the Central Treaty Organization (CENTO), after a Mutual Defence Assistance Agreement signed in May 1954, which facilitated increased levels of both economic and military aid from the U.S.\n",
      "In 1958, Ayub Khan led Pakistan's first military coup, becoming Chief Martial Law Administrator (CMLA) and later President until 1969. During his tenure, the U.S. delivered substantial economic and military aid, despite Pakistan's governance by military regime, and amidst events like the Indo-Pakistani war of 1965.\n",
      "Yahya Khan succeeded Ayub in 1969, holding power during the Indo-Pakistani war of 1971 which led to the secession of East Pakistan and the formation of Bangladesh. Under Yahya, the U.S. provided adequate economic but minimal military aid.\n",
      "Civilian governance of Zulfikar Ali Bhutto resumed from 1971 to 1977, during which the U.S. offered modest economic support and withheld military aid as Pakistan finalized its constitution, establishing a parliamentary democracy.\n",
      "Following another military coup in 1977, Muhammad Zia-ul-Haq led the country. In April 1979, President Jimmy Carter halted all aid, excluding food assistance, due to Pakistan's efforts to establish a uranium enrichment facility, following Symington Amendment. Initial U.S. aid was limited, increasing significantly after geopolitical shifts such as the Soviet invasion of Afghanistan in 1979 and the fall of the Shah of Iran. U.S. sanctions imposed in April 1979 due to Pakistan's nuclear activities were lifted by the end of the year in light of these events.\n",
      "Between 1988 and 1999, under civilian and democratic governments, U.S. aid was low, particularly after the Soviet withdrawal from Afghanistan in 1989. The aid was suspended in the 1990s under President George H. W. Bush, who cited concerns over Pakistan's developing nuclear program. Relations between the two countries deteriorated as the U.S. implemented the Pressler Amendment. This amendment led to severe sanctions against Pakistan, exacerbating economic challenges for the country's nascent civilian government. Consequently, all forms of bilateral aid from the U.S. to Pakistan were halted. The once expansive operations of the U.S. Agency for International Development (USAID) in Pakistan, which had employed over 1,000 staff across the country, were dramatically reduced almost overnight. Further complications arose from U.S. sanctions following Pakistan's nuclear tests in 1998, which were conducted in response to similar tests by India.\n",
      "The return of military rule under Pervez Musharraf from 1999 to 2008 initially saw little U.S. economic or military aid. However, following the September 11, 2001 attacks, all U.S. sanctions were removed, and Pakistan, having aligned with the U.S. in the War on Terror, received substantial increases in both economic and military assistance.\n",
      "On June 16, 2009, the U.S. Senate Foreign Relations Committee passed the Enhanced Partnership with Pakistan Act of 2009, commonly referred to as the Kerry-Lugar Bill. The bipartisan act authorized an annual provision of $1.5 billion in U.S. aid to Pakistan, aimed at fostering enhanced bilateral relations.\n",
      "In 2011, the Obama administration suspended more than one-third of all military assistance, totaling approximately $800 million due to Osama bin Laden-related controversy. This reduction encompassed funds designated for military hardw...\n",
      "\n",
      "--- Result 2 ---\n",
      "Content:\n",
      "The Dominion of Pakistan, officially Pakistan, was an independent federal dominion in the British Commonwealth of Nations, which existed from 14 August 1947 to 23 March 1956. It was created by the passing of the Indian Independence Act 1947 by the British parliament, which also created an independent Dominion of India.\n",
      "The new dominion consisted of those presidencies and provinces of British India which were allocated to it in the Partition of India. Until 1947, these regions had been ruled by the United Kingdom as a part of the British Empire.\n",
      "Its status as a federal dominion within the British Empire ended in 1956 with the completion of the Constitution of Pakistan, which established the country as a republic. The constitution also administratively split the nation into West Pakistan and East Pakistan. Until then, these provinces had been governed as a singular entity, despite being separate geographic exclaves. Eventually, the East became Bangladesh and the West became Pakistan.\n",
      "During the year that followed its independence, the new country was joined by the princely states of Pakistan, which were ruled by princes who had previously been in subsidiary alliances with the British. These states acceded to Pakistan one by one as their rulers signed Instruments of Accession. For many years, these states enjoyed a special status within the dominion and later the republic, but they were slowly incorporated into the provinces. The last remnants of their internal self-government were lost by 1974.\n",
      "\n",
      "\n",
      "== History ==\n",
      "\n",
      "\n",
      "=== Partition and independence ===\n",
      "\n",
      "Section 1 of the Indian Independence Act 1947 provided that from \"the fifteenth day of August, nineteen hundred and forty-seven, two independent dominions shall be set up in India, to be known respectively as India and Pakistan.\"  Muslims had been pushing for their own state since at least 1940 (see the Lahore resolution), believing they would become second-class citizens in a Hindu-majority India otherwise. The British monarch became the head of state of both the new dominions, with Pakistan sharing a king with the United Kingdom and the other dominions of the British Commonwealth, and the monarch's constitutional roles in Pakistan were delegated to the Governor-General of Pakistan.\n",
      "Before August 1947, about half of the area of present-day Pakistan was part of British India, which was directly governed by the British in the name of the British Crown, while the remainder were princely states in subsidiary alliances with the British, enjoying semi-autonomous self-government. The British abandoned these alliances in August 1947, leaving the states entirely independent, and between 1947 and 1948 the states all acceded to Pakistan, while retaining internal self-government for several years.\n",
      "More than ten million people migrated across the new borders and between 200,000 and 2,000,000 people died in the spate of communal violence in the Punjab in what some scholars have described as a 'retributive genocide' between the religions. The Pakistani government claimed that 50,000 Muslim women were abducted and raped by Hindu and Sikh men and similarly the Indian government claimed that Muslims abducted and raped 33,000 Hindu and Sikh women. The two governments agreed to repatriate abducted women and thousands of Hindu, Sikh and Muslim women were repatriated to their families in the 1950s. The dispute over Kashmir escalated into the first war between India and Pakistan. With the assistance of the United Nations (UN) the war was ended but it became the Kashmir dispute, unresolved as of 2024.\n",
      "\n",
      "In 1947, the founding fathers of Pakistan agreed to appoint Liaquat Ali Khan as the country's first prime minister, with Muhammad Ali Jinnah as both first governor-general and speaker of the State Parliament. Mountbatten had offered to serve as Governor-general of both India and Pakistan but Jinnah refused this offer.\n",
      "The first formal step to transform Pakistan into an ideological Islamic state was taken in Ma...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "# Initialize the retriever (optional: set language and top_k)\n",
    "retriever = WikipediaRetriever(top_k_results=2, lang=\"en\")\n",
    "\n",
    "# Define your query\n",
    "query = \"the geopolitical history of india and pakistan from the perspective of a chinese\"\n",
    "\n",
    "# Get relevant Wikipedia documents\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# Print retrieved content\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Content:\\n{doc.page_content}...\")  # truncate for display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64721c14",
   "metadata": {},
   "source": [
    "# Vector Store Retreiver \n",
    "\n",
    "# Extract information from vector strore database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bbca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shoeb/Desktop/VS_Code/Gen_AI/venv/lib/python3.13/site-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "Chroma is a vector database optimized for LLM-based search.\n",
      "\n",
      "--- Result 2 ---\n",
      "Chroma is a vector database optimized for LLM-based search.\n",
      "\n",
      "--- Result 1 ---\n",
      "Chroma is a vector database optimized for LLM-based search.\n",
      "\n",
      "--- Result 2 ---\n",
      "Chroma is a vector database optimized for LLM-based search.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Step 1: Your source documents\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain helps developers build LLM applications easily.\"),\n",
    "    Document(page_content=\"Chroma is a vector database optimized for LLM-based search.\"),\n",
    "    Document(page_content=\"Embeddings convert text into high-dimensional vectors.\"),\n",
    "    Document(page_content=\"OpenAI provides powerful embedding models.\"),\n",
    "]\n",
    "\n",
    "\n",
    "# Step 2: Initialize embedding model\n",
    "embedding_model = MistralAIEmbeddings()\n",
    "\n",
    "# Step 3: Create Chroma vector store in memory\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"my_collection\"\n",
    ")\n",
    "\n",
    "# Step 4: Convert vectorstore into a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "\n",
    "query = \"What is Chroma used for?\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc.page_content)\n",
    "    \n",
    "    \n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79dabc",
   "metadata": {},
   "source": [
    "## MMR \n",
    "\n",
    "## üîç What is MMRRetriever?\n",
    "\n",
    "**MMR (Maximal Marginal Relevance)** is a technique used to select results that are both:\n",
    "\n",
    "* **Relevant** to the query (high similarity)\n",
    "* **Diverse** from each other (low redundancy)\n",
    "\n",
    "This is useful in **retrieval-augmented generation (RAG)** to avoid retrieving multiple similar documents that all say the same thing.\n",
    "\n",
    "## ‚öôÔ∏è `lambda_mult`: How it works\n",
    "\n",
    "```python\n",
    "lambda_mult: float = 0.5\n",
    "```\n",
    "\n",
    "This parameter **controls the trade-off**:\n",
    "\n",
    "* **High `lambda_mult` (\\~1.0)** ‚Üí more focus on **relevance**\n",
    "* **Low `lambda_mult` (\\~0.0)** ‚Üí more focus on **diversity**\n",
    "* **Balanced (`0.5`)** ‚Üí equal weight to both\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| `lambda_mult` Value | Behavior                      | Use Case                                 |\n",
    "| ------------------- | ----------------------------- | ---------------------------------------- |\n",
    "| \\~1.0               | Max relevance, less diversity | Precise answers, e.g., QA systems        |\n",
    "| \\~0.0               | Max diversity, less relevance | Broad coverage, e.g., exploratory search |\n",
    "| 0.5                 | Balanced approach             | Default setting for RAG pipelines        |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Best Practice\n",
    "\n",
    "Start with `lambda_mult = 0.5`, and:\n",
    "\n",
    "* Increase if you're seeing **too much variety**\n",
    "* Decrease if you're seeing **too many similar results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "623401d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shoeb/Desktop/VS_Code/Gen_AI/venv/lib/python3.13/site-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "LangChain is used to build LLM based applications.\n",
      "\n",
      "--- Result 2 ---\n",
      "LangChain makes it easy to work with LLMs.\n",
      "\n",
      "--- Result 3 ---\n",
      "LangChain supports Chroma, FAISS, Pinecone, and more.\n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain makes it easy to work with LLMs.\"),\n",
    "    Document(page_content=\"LangChain is used to build LLM based applications.\"),\n",
    "    Document(page_content=\"Chroma is used to store and search document embeddings.\"),\n",
    "    Document(page_content=\"Embeddings are vector representations of text.\"),\n",
    "    Document(page_content=\"MMR helps you get diverse results when doing similarity search.\"),\n",
    "    Document(page_content=\"LangChain supports Chroma, FAISS, Pinecone, and more.\"),\n",
    "]\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embedding_model = MistralAIEmbeddings()\n",
    "\n",
    "# Step 2: Create the FAISS vector store from documents\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "# Enable MMR in the retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",                   # <-- This enables MMR\n",
    "    search_kwargs={\"k\": 3, \"lambda_mult\": 1.0}  # k = top results, lambda_mult = relevance-diversity balance\n",
    ")\n",
    "\n",
    "query = \"What is langchain?\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc.page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a4905",
   "metadata": {},
   "source": [
    "## Multi query retreiver \n",
    "- Improve query quality. \n",
    "- By sending the original query to LLM, LLm in turn genrates multiple queries that are semantically similar to the original query.\n",
    "- Then it retreives the results for each of the generated queries and returns the top results across all queries, post remvoing the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccc53fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shoeb/Desktop/VS_Code/Gen_AI/venv/lib/python3.13/site-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "Drinking sufficient water throughout the day helps maintain metabolism and energy.\n",
      "\n",
      "--- Result 2 ---\n",
      "Mindfulness and controlled breathing lower cortisol and improve mental clarity.\n",
      "\n",
      "--- Result 3 ---\n",
      "The solar energy system in modern homes helps balance electricity demand.\n",
      "\n",
      "--- Result 4 ---\n",
      "Consuming leafy greens and fruits helps detox the body and improve longevity.\n",
      "\n",
      "--- Result 5 ---\n",
      "Deep sleep is crucial for cellular repair and emotional regulation.\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "--- Result 1 ---\n",
      "Drinking sufficient water throughout the day helps maintain metabolism and energy.\n",
      "\n",
      "--- Result 2 ---\n",
      "Mindfulness and controlled breathing lower cortisol and improve mental clarity.\n",
      "\n",
      "--- Result 3 ---\n",
      "The solar energy system in modern homes helps balance electricity demand.\n",
      "\n",
      "--- Result 4 ---\n",
      "Consuming leafy greens and fruits helps detox the body and improve longevity.\n",
      "\n",
      "--- Result 5 ---\n",
      "Regular walking boosts heart health and can reduce symptoms of depression.\n",
      "\n",
      "--- Result 6 ---\n",
      "Black holes bend spacetime and store immense gravitational energy.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Relevant health & wellness documents\n",
    "all_docs = [\n",
    "    Document(page_content=\"Regular walking boosts heart health and can reduce symptoms of depression.\", metadata={\"source\": \"H1\"}),\n",
    "    Document(page_content=\"Consuming leafy greens and fruits helps detox the body and improve longevity.\", metadata={\"source\": \"H2\"}),\n",
    "    Document(page_content=\"Deep sleep is crucial for cellular repair and emotional regulation.\", metadata={\"source\": \"H3\"}),\n",
    "    Document(page_content=\"Mindfulness and controlled breathing lower cortisol and improve mental clarity.\", metadata={\"source\": \"H4\"}),\n",
    "    Document(page_content=\"Drinking sufficient water throughout the day helps maintain metabolism and energy.\", metadata={\"source\": \"H5\"}),\n",
    "    Document(page_content=\"The solar energy system in modern homes helps balance electricity demand.\", metadata={\"source\": \"I1\"}),\n",
    "    Document(page_content=\"Python balances readability with power, making it a popular system design language.\", metadata={\"source\": \"I2\"}),\n",
    "    Document(page_content=\"Photosynthesis enables plants to produce energy by converting sunlight.\", metadata={\"source\": \"I3\"}),\n",
    "    Document(page_content=\"The 2022 FIFA World Cup was held in Qatar and drew global energy and excitement.\", metadata={\"source\": \"I4\"}),\n",
    "    Document(page_content=\"Black holes bend spacetime and store immense gravitational energy.\", metadata={\"source\": \"I5\"}),\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embedding_model = MistralAIEmbeddings()\n",
    "\n",
    "# LLM for multi-query retriever\n",
    "llm = ChatMistralAI(model_name='mistral-large-latest')\n",
    "\n",
    "# Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=all_docs, \n",
    "    embedding=embedding_model)\n",
    "\n",
    "# Create similariy retrievers\n",
    "similarity_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# Create multi-query retriever\n",
    "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "\n",
    "# Query\n",
    "query = \"How to improve energy levels and maintain balance?\"\n",
    "\n",
    "\n",
    "# Retrieve results\n",
    "similarity_results = similarity_retriever.invoke(query)\n",
    "multiquery_results= multiquery_retriever.invoke(query)\n",
    "\n",
    "\n",
    "\n",
    "for i, doc in enumerate(similarity_results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc.page_content)\n",
    "\n",
    "print(\"*\"*150)\n",
    "\n",
    "for i, doc in enumerate(multiquery_results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6334f",
   "metadata": {},
   "source": [
    "# ContextualCompressionRetriever\n",
    "- Removes irrelevant documents from retreved documents of user query\n",
    "- All retreived documents documents passed to the LLM and in return it removes all irrelevant docs andreturn back to the reteiver.\n",
    "- Base retreiver : - To retreive all docs (including irreleavnt)\n",
    "- Compressor : LLM  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4b4f30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shoeb/Desktop/VS_Code/Gen_AI/venv/lib/python3.13/site-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "Photosynthesis is the process by which green plants convert sunlight into energy.\n",
      "\n",
      "--- Result 2 ---\n",
      "The chlorophyll in plant cells captures sunlight during photosynthesis.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_mistralai import MistralAIEmbeddings, ChatMistralAI\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Recreate the document objects from the previous data\n",
    "docs = [\n",
    "    Document(page_content=(\n",
    "        \"\"\"The Grand Canyon is one of the most visited natural wonders in the world.\n",
    "        Photosynthesis is the process by which green plants convert sunlight into energy.\n",
    "        Millions of tourists travel to see it every year. The rocks date back millions of years.\"\"\"\n",
    "    ), metadata={\"source\": \"Doc1\"}),\n",
    "\n",
    "    Document(page_content=(\n",
    "        \"\"\"In medieval Europe, castles were built primarily for defense.\n",
    "        The chlorophyll in plant cells captures sunlight during photosynthesis.\n",
    "        Knights wore armor made of metal. Siege weapons were often used to breach castle walls.\"\"\"\n",
    "    ), metadata={\"source\": \"Doc2\"}),\n",
    "\n",
    "    Document(page_content=(\n",
    "        \"\"\"Basketball was invented by Dr. James Naismith in the late 19th century.\n",
    "        It was originally played with a soccer ball and peach baskets. NBA is now a global league.\"\"\"\n",
    "    ), metadata={\"source\": \"Doc3\"}),\n",
    "\n",
    "    Document(page_content=(\n",
    "        \"\"\"The history of cinema began in the late 1800s. Silent films were the earliest form.\n",
    "        Thomas Edison was among the pioneers. Photosynthesis does not occur in animal cells.\n",
    "        Modern filmmaking involves complex CGI and sound design.\"\"\"\n",
    "    ), metadata={\"source\": \"Doc4\"})\n",
    "]\n",
    "\n",
    "\n",
    "# Create a FAISS vector store from the documents\n",
    "embedding_model = MistralAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Create a base retriever from the vector store\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Set up the compressor using an LLM\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "\n",
    "# Create the contextual compression retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=base_retriever,\n",
    "    base_compressor=compressor\n",
    ")\n",
    "\n",
    "\n",
    "# Query the retriever\n",
    "query = \"What is photosynthesis?\"\n",
    "compressed_results = compression_retriever.invoke(query)\n",
    "\n",
    "\n",
    "for i, doc in enumerate(compressed_results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc.page_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
